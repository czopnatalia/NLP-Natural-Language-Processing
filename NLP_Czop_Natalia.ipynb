{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "5tQ1rF2IHCT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP (ang. Natural Language Processing), czyli Przetwarzanie Języka Naturalnego. Jej głównym celem jest umożliwienie komputerom rozumienia, interpretowania i generowania ludzkiego języka w sposób, który jest wartościowy i sensowny. NLP pozwala na automatyczne przeszukiwanie tysięcy raportów.  \n",
        "\n",
        "Cele projektu:\n",
        "- Poznanie podstaw przetwarzania języka naturalnego (NLP) – jak analizować raporty geologiczne lub dane z misji kosmicznych.\n",
        "\n",
        "- Praktyczna nauka tokenizacji, pad_sequences i embeddingów w Keras/TensorFlow.\n",
        "\n",
        "- Tworzenie prostego modelu klasyfikacji tekstu, np. czy raport dotyczy złóż rud, minerałów czy eksploracji kosmosu.\n",
        "\n",
        "- Zastosowanie Pipeline do automatyzacji przetwarzania danych.\n",
        "\n",
        "- Analiza wyników i refleksja nad interpretacją modelu w kontekście Nauk o Ziemii."
      ],
      "metadata": {
        "id": "8NAbXlDaHKya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Symulacja raportu geologicznego lub wyników misji kosmicznej - lista zdań"
      ],
      "metadata": {
        "id": "c7LqYInhIUcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U0nK16Ec9deE"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    \"Odkryto złoża żelaza w warstwach osadowych\",\n",
        "    \"Badania geofizyczne wskazują anomalię magnetyczną\",\n",
        "    \"Misja kosmiczna potwierdziła obecność minerałów na Marsie\",\n",
        "    \"Wiercenia w rejonie rud miedzi wykazały wysoką koncentrację metalu\",\n",
        "    \"Sonda wylądowała na powierzchni krateru uderzeniowego na Księżycu\",\n",
        "    \"Analiza próbek skał wskazuje na wysoką zawartość krzemianów i bazaltu\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Najważniejsze słowa kluczowe do klasyfikacji:  \n",
        "- kosmos: [\"misja\", \"Mars\", \"sonda\", \"asteroidzie\"],  \n",
        "- geologia: [\"złoża\", \"wiercenia\", \"rudy\", \"geofizyczne\"]."
      ],
      "metadata": {
        "id": "lh01NNuhH-vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizacja\n",
        "Tokenizacja to proces, w którym każde słowo w zdaniu jest przypisywane do unikalnej liczby całkowitej, aby komputer mógł je przetwarzać. Zdania zamieniane są na listy liczb."
      ],
      "metadata": {
        "id": "jx93d-4zIWKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5ok1vpPIXsz",
        "outputId": "c47140cd-dab2-4322-acd0-964e850a21eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'na': 1, 'w': 2, 'wysoką': 3, 'odkryto': 4, 'złoża': 5, 'żelaza': 6, 'warstwach': 7, 'osadowych': 8, 'badania': 9, 'geofizyczne': 10, 'wskazują': 11, 'anomalię': 12, 'magnetyczną': 13, 'misja': 14, 'kosmiczna': 15, 'potwierdziła': 16, 'obecność': 17, 'minerałów': 18, 'marsie': 19, 'wiercenia': 20, 'rejonie': 21, 'rud': 22, 'miedzi': 23, 'wykazały': 24, 'koncentrację': 25, 'metalu': 26, 'sonda': 27, 'wylądowała': 28, 'powierzchni': 29, 'krateru': 30, 'uderzeniowego': 31, 'księżycu': 32, 'analiza': 33, 'próbek': 34, 'skał': 35, 'wskazuje': 36, 'zawartość': 37, 'krzemianów': 38, 'i': 39, 'bazaltu': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jakie słowo otrzymało najmniejszy indeks?** - Słowa występujące najczęściej (np. spójniki \"na\", \"w\") otrzymują zazwyczaj najniższe indeksy. Unikalne terminy, nie występujące w innych zdaniach (np. \"bazalt\"), otrzymują najwyższe indeksy w słowniku."
      ],
      "metadata": {
        "id": "_tA89631J2t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zamiana tekstu na sekwencje liczbowych tokenów"
      ],
      "metadata": {
        "id": "ma5KeMjIKIk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skHtqdVdKJjS",
        "outputId": "14fa9896-e3f1-42bb-aa6f-0d0343f8c59a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 5, 6, 2, 7, 8], [9, 10, 11, 12, 13], [14, 15, 16, 17, 18, 1, 19], [20, 2, 21, 22, 23, 24, 3, 25, 26], [27, 28, 1, 29, 30, 31, 1, 32], [33, 34, 35, 36, 1, 3, 37, 38, 39, 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Każde zdanie jest teraz listą liczb całkowitych.  \n",
        "Każda liczba odpowiada słowu w słowniku (tokenizer.word_index).  \n",
        "Sekwencje mają różną długość – dokładnie taką, ile było słów w oryginalnym zdaniu.  \n",
        "Odkryto złoża żelaza w warstwach osadowych -> [4, 5, 6, 2, 7, 8]"
      ],
      "metadata": {
        "id": "BAV2h_OiKU-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding (Wyrównywanie długości)\n",
        "Modele deep learningowe (=neuronowe) wymagają, aby wszystkie sekwencje wejściowe miały taką samą długość.\n",
        "Dlatego stosujemy padding, czyli dopasowujemy krótsze sekwencje do długości najdłuższej (lub ustalonej)."
      ],
      "metadata": {
        "id": "hG8PxNHzK3uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhYfpBvuLB0b",
        "outputId": "d70a01f1-8596-43be-ce49-b99a0ad4102f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  5  6  2  7  8  0  0  0  0]\n",
            " [ 9 10 11 12 13  0  0  0  0  0]\n",
            " [14 15 16 17 18  1 19  0  0  0]\n",
            " [20  2 21 22 23 24  3 25 26  0]\n",
            " [27 28  1 29 30 31  1 32  0  0]\n",
            " [33 34 35 36  1  3 37 38 39 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Jak zmieniła się długość?** Wszystkie sekwencje mają teraz długość najdłuższego zdania w zbiorze (10 elementów).  \n",
        "- **Jakie liczby reprezentują brakujące miejsca (padding)?** Jest to liczba 0.\n",
        "- **Dlaczego takie uzupełnienie jest potrzebne w modelach sieci neuronowych?** Warstwy wejściowe sieci neuronowych (np. Dense) wymagają macierzy o stałych wymiarach (tensors). Sieć nie potrafi \"sama z siebie\" przyjąć raz wektora 5-elementowego, a raz 10-elementowego w tej samej warstwie."
      ],
      "metadata": {
        "id": "HGBJlgFnLH6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tworzenie i trenowanie modelu"
      ],
      "metadata": {
        "id": "gMywQM8GTLwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Rozszerzone dane\n",
        "texts = [\n",
        "    \"Odkryto złoża żelaza w warstwach osadowych\",\n",
        "    \"Wiercenia w rejonie rud miedzi wykazały wysoką koncentrację metalu\",\n",
        "    \"Badania geofizyczne wykazały anomalię magnetyczną\",\n",
        "    \"Misja kosmiczna potwierdziła obecność minerałów na Marsie\",\n",
        "    \"Sonda wylądowała na powierzchni krateru\",\n",
        "    \"Orbiter krąży wokół nowej egzoplanety\"\n",
        "]\n",
        "labels = np.array([0, 0, 0, 1, 1, 1]) # 0-geologia, 1-kosmos\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_length = max([len(x) for x in sequences])\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=100, output_dim=8, input_length=max_length),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid') # dla klasyfikacji binarnej\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(padded, labels, epochs=20, verbose=0)\n",
        "\n",
        "predictions = model.predict(padded)\n",
        "for i in range(len(texts)):\n",
        "    print(f\"Tekst: {texts[i]}\")\n",
        "    print(f\"Prawdopodobieństwo: {predictions[i][0]:.4f} (Klasa: {labels[i]})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwMtbAXTNJR",
        "outputId": "b32d54d5-53df-4a97-c65e-662f04f0ab77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "Tekst: Odkryto złoża żelaza w warstwach osadowych\n",
            "Prawdopodobieństwo: 0.4743 (Klasa: 0)\n",
            "\n",
            "Tekst: Wiercenia w rejonie rud miedzi wykazały wysoką koncentrację metalu\n",
            "Prawdopodobieństwo: 0.4599 (Klasa: 0)\n",
            "\n",
            "Tekst: Badania geofizyczne wykazały anomalię magnetyczną\n",
            "Prawdopodobieństwo: 0.4674 (Klasa: 0)\n",
            "\n",
            "Tekst: Misja kosmiczna potwierdziła obecność minerałów na Marsie\n",
            "Prawdopodobieństwo: 0.5354 (Klasa: 1)\n",
            "\n",
            "Tekst: Sonda wylądowała na powierzchni krateru\n",
            "Prawdopodobieństwo: 0.5273 (Klasa: 1)\n",
            "\n",
            "Tekst: Orbiter krąży wokół nowej egzoplanety\n",
            "Prawdopodobieństwo: 0.5381 (Klasa: 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jak embeddingi reprezentują słowa w przestrzeni liczbowej?**  \n",
        "\n",
        "Embeddingi zamieniają słowa na współrzędne w wielowymiarowej przestrzeni, gdzie odległość między punktami odpowiada podobieństwu znaczeniowemu słów. Zamiast reprezentować słowo jako zwykły numer (jak w tokenizacji, gdzie \"Mars\" = 5, a \"Ziemia\" = 6), embedding zamienia je w wektor, czyli listę liczb zmiennoprzecinkowych (np. $[0.12, -0.54, 0.89, \\dots]$).  \n",
        "\n",
        "Najważniejszą cechą embeddingów jest to, że słowa, które występują w podobnych kontekstach, lądują blisko siebie w tej przestrzeni.\n",
        "\n",
        "- Słowa \"żelazo\" i \"miedź\" będą miały podobne współrzędne, bo oba są metalami i pojawiają się w raportach geologicznych.\n",
        "\n",
        "- Słowo \"orbita\" będzie znajdować się w zupełnie innej części tej przestrzeni, blisko słów \"planeta\" czy \"satelita\".  \n",
        "\n",
        "**Czy model poprawnie odróżnia raporty geologiczne od kosmicznych?**  \n",
        "\n",
        "Model z warstwą Dense(1, activation='sigmoid') zwraca wartość od 0 do 1:\n",
        "\n",
        "- Wynik blisko 0.0 (np. 0.02) oznacza dużą pewność, że to geologia.\n",
        "\n",
        "- Wynik blisko 1.0 (np. 0.98) oznacza dużą pewność, że to kosmos.\n",
        "\n",
        "- Wynik w okolicy 0.5 oznacza, że model jest zdezorientowany i nie widzi wystarczającej liczby słów kluczowych, które zna z etapu nauki.  \n",
        "\n",
        "Wszystkie wyniki oscylują wokół 0.50 (różnice są na poziomie zaledwie 3-7%). Oznacza to, że model jest bardzo niepewny."
      ],
      "metadata": {
        "id": "pmKnEmLQT0pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testowanie nowego zdania"
      ],
      "metadata": {
        "id": "wLT2P5zHUE-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = [\"Nowe badania wskazują złoża niklu na asteroidzie\"]\n",
        "seq = tokenizer.texts_to_sequences(test_text)\n",
        "padded_test = pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model.predict(padded_test)\n",
        "print(f\"Wynik (blisko 0 = geologia, blisko 1 = kosmos): {prediction[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qet1YCHhUIee",
        "outputId": "c043f611-fb09-4c6e-fa88-54332217f611"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Wynik (blisko 0 = geologia, blisko 1 = kosmos): 0.4876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Czy model prawidłowo sklasyfikował zdanie?**  \n",
        "\n",
        "Nie, model nie sklasyfikował tego zdania prawidłowo.  \n",
        "\n",
        "Wynik ten znajduje się niemal dokładnie pośrodku skali (0.5). Oznacza to, że model jest „zdezorientowany” i w praktyce zgaduje. Dla zdania o asteroidzie (klasa 1 - kosmos), oczekiwalibyśmy wyniku powyżej 0.80.\n",
        "\n",
        "**Jak można poprawić działanie modelu przy nowych słowach?**  \n",
        "\n",
        "Aby model lepiej radził sobie z nowymi słowami (np. \"nikiel\", jeśli go wcześniej nie było), należy trenować na znacznie większym korpusie tekstów.\n",
        "- Przy tworzeniu Tokenizera warto dodać specjalny znacznik dla nieznanych słów. Dzięki temu model uczy się, że istnieją słowa spoza słownika i może przypisać im pewne znaczenie kontekstowe.\n",
        "- Użycie Pre-trained Embeddings (np. Word2Vec lub GloVe): Zamiast uczyć się znaczenia słów od zera, można użyć gotowych \"map\" słów wygenerowanych na milionach dokumentów\n",
        "- Lematyzacja i Stemming: Sprowadzenie słów do formy podstawowej. Dla komputera \"złoża\", \"złożu\" i \"złoża\" to trzy różne liczby. Po lematyzacji wszystkie stają się słowem \"złoże\". To drastycznie zmniejsza rozmiar słownika i ułatwia naukę.\n",
        "- Augmentacja danych: Dodanie większej liczby przykładów. W NLP można to robić np. poprzez zamianę słów na synonimy (np. zamiast \"złoża żelaza\" – \"pokłady ferrytu\")."
      ],
      "metadata": {
        "id": "5_ccvzG_Uaul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline i automatyzacja"
      ],
      "metadata": {
        "id": "Ekvv_wCJUiu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "def tokenize_pad(X):\n",
        "    # Ważne: używamy tokenizer.texts_to_sequences zdefiniowanego wcześniej\n",
        "    seq = tokenizer.texts_to_sequences(X)\n",
        "    return pad_sequences(seq, maxlen=max_length, padding='post')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tokenize_pad', FunctionTransformer(tokenize_pad, validate=False)),\n",
        "    ('model', model)\n",
        "])"
      ],
      "metadata": {
        "id": "7LYJvmqxUjyD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jak pipeline chroni przed błędami przy nowych słowach?**  \n",
        "\n",
        "1. Spójność transformacji: Pipeline chroni przed błędami, bo zapewnia, że nowe dane testowe przejdą identyczną ścieżkę transformacji (taka sama długość, ten sam słownik), co dane treningowe.\n",
        "2. Izolacja słownika (Data Leakage): Pipeline wymusza, aby transformacja danych testowych opierała się wyłącznie na słowniku (word_index), który został zbudowany podczas treningu. Dzięki temu mamy realistyczny obraz tego, jak model radzi sobie z nieznanymi słowami.\n",
        "3. Obsługa słów spoza słownika (OOV): Jeśli w nowym zdaniu pojawi się słowo, którego nie było w treningu to dzięki FunctionTransformer wewnątrz Pipeline'u, proces texts_to_sequences po prostu pominie nieznane słowo (lub zamieni je na token <OOV>, jeśli go skonfigurowano).\n",
        "4. Automatyzacja \"czyszczenia\": Jeśli w przyszłości zdecydujemy się dodać krok usuwania znaków interpunkcyjnych lub zamiany liter na małe, robimy to w jednym miejscu w Pipeline. Mamy wtedy pewność, że nowe dane testowe nie zostaną odrzucone tylko dlatego, że ktoś wpisał zdanie wielkimi literami."
      ],
      "metadata": {
        "id": "utL3GlN-UtD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test pipeline'u"
      ],
      "metadata": {
        "id": "BAI7Hg4eU9u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_texts = [\"Eksploracja Marsa wykazała obecność żelaza i niklu\"]\n",
        "print(f\"Wynik (blisko 0 = geologia, blisko 1 = kosmos): {pipeline.predict(new_texts)[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ArDEagBVCZg",
        "outputId": "76a16801-ced2-4ce8-a894-ccd9f049a556"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Wynik (blisko 0 = geologia, blisko 1 = kosmos): 0.4940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Czy pipeline poprawnie przetworzył dane i dokonał predykcji?**  \n",
        "\n",
        "Wynik 0.4940 ponownie wskazuje na to, że model jest całkowicie niepewny (wynik niemal idealnie pośrodku). Choć technicznie Pipeline zadziałał poprawnie (przyjął tekst, zamienił go na liczby i przeszedł przez model bez błędu), to sama predykcja jest bezużyteczna.  \n",
        "\n",
        "**Jak można rozbudować pipeline o preprocessing, np. usuwanie stop-words lub stemming?**  \n",
        "\n",
        "Aby model działał lepiej, musimy \"wyczyścić\" tekst, zanim trafi do modelu. Możemy to zrobić, dodając nową funkcję do naszego Pipeline'u.\n",
        "\n",
        "1. Usuwanie Stop-words (słów nieznaczących)\n",
        "Słowa takie jak \"i\", \"w\", \"na\", \"z\" występują w obu kategoriach raportów. Ich usunięcie pozwala modelowi skupić się na \"mięsie\" (słowach kluczowych).\n",
        "\n",
        "2. Stemming / Lematyzacja\n",
        "Dla modelu \"Marsa\" i \"Marsie\" to dwa różne słowa. Stemming ucina końcówki, sprowadzając je do wspólnego rdzenia \"Mars\".  \n",
        "\n",
        "3. Usuwanie znaków interpunkcyjnych  \n",
        "\n",
        "4. Usuwanie różnicy między wielkością liter"
      ],
      "metadata": {
        "id": "fR0ZpVHGVS5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rozbudowany Pipeline"
      ],
      "metadata": {
        "id": "vSjhT9KacUxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Przykładowa lista polskich stop-words\n",
        "STOP_WORDS = {\"i\", \"w\", \"na\", \"z\", \"o\", \"u\", \"pod\", \"ponad\", \"wykazała\", \"obecność\"}\n",
        "\n",
        "def clean_text(texts):\n",
        "    cleaned_texts = []\n",
        "    for text in texts:\n",
        "        # 1. Małe litery\n",
        "        text = text.lower()\n",
        "        # 2. Usuwanie znaków interpunkcyjnych\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        # 3. Usuwanie stop-words\n",
        "        text = \" \".join([word for word in text.split() if word not in STOP_WORDS])\n",
        "        cleaned_texts.append(text)\n",
        "    return cleaned_texts\n",
        "\n",
        "# Nowy, rozbudowany Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('text_cleaning', FunctionTransformer(clean_text, validate=False)),\n",
        "    ('tokenize_pad', FunctionTransformer(tokenize_pad, validate=False)),\n",
        "    ('model', model)\n",
        "])"
      ],
      "metadata": {
        "id": "O-mYlYaxcZta"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}